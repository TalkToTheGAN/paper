\documentclass[11pt, english]{article}
  
  % if you need to pass options to natbib, use, e.g.:
  % \PassOptionsToPackage{numbers, compress}{natbib}
  % before loading nips_2016
  %
  % to avoid loading the natbib package, add option nonatbib:
  %\usepackage[no]{nips_2016}
  
  %\usepackage{nips_2016}
  
  % to compile a camera-ready version, add the [final] option, e.g.:
  \usepackage[final]{nips_2016}
  \usepackage{natbib}
  \setcitestyle{numbers}
  \usepackage[utf8]{inputenc} % allow utf-8 input
  \usepackage[T1]{fontenc}    % use 8-bit T1 fonts
  \usepackage{hyperref}       % hyperlinks
  \usepackage{url}            % simple URL typesetting
  \usepackage{booktabs}       % professional-quality tables
  \usepackage{amsfonts}       % blackboard math symbols
  \usepackage{nicefrac}       % compact symbols for 1/2, etc.
  \usepackage{microtype}      % microtypography
  \usepackage{indentfirst}
  \usepackage{amsmath}
  \usepackage{fancyhdr}
  \usepackage{graphicx}
  \usepackage{printlen}
  \usepackage{color}
  \usepackage{babel}
  %\usepackage{lmodern}
  
  \graphicspath{{./figures/}}
  
  \fancypagestyle{equalc}{\fancyhf{}\renewcommand{\headrulewidth}{0pt}\fancyfoot[R]{* indicates equal contribution}}
  
  \title{CSC2547 Learning Discrete Latent Structure: Generative Adversarial Networks via Gradient Estimators}
  
  % The \author macro works with any number of authors. There are two
  % commands used to separate the names and addresses of multiple
  % authors: \And and \AND.
  %
  % Using \And between authors leaves it to LaTeX to determine where to
  % break the lines. Using \AND forces a line break at that point. So,
  % if LaTeX puts 3 of 4 authors names on the first line, and the last
  % on the second line, try using \AND instead of \And before the third
  % author name.
  
  \author{
  	\textbf{Aparna Balagopalan*}\\
	University of Toronto\\
	27 King's College Circle\\
	Toronto, ON M5S\\
	aparna@cs.toronto.edu
	\and
  	\textbf{Satya Krishna Gorti*}\\
	University of Toronto\\
	27 King's College Circle\\
	Toronto, ON M5S\\
	satyag@cs.toronto.edu
	\and
	\textbf{Mathieu Ravaut*} \\ 
	University of Toronto \\
	27 King's College Circle\\
	Toronto, ON M5S\\
	mravox@cs.toronto.edu 
	\and
  	\textbf{Raeid Saqur*}\\
	University of Toronto\\
	27 King's College Circle\\
	Toronto, ON M5S\\
	raeidsaqur@cs.toronto.edu	
}
  
  \begin{document}
  % \nipsfinalcopy is no longer used
  
  
  \maketitle
  
  \begin{abstract}
  In this work, we consider the problem of sequence generation using Generative Adversarial Networks (GANs). Sequential data generation using Generative Adversarial Networks presents two structural issues: backpropagating gradients through discrete generator outputs, and waiting for an entire output sequence to be generated before computing the performance score. In this project, we aim to circumvent the above mentioned issues using policy gradients. We build on SeqGAN and compare the performance of the following gradient estimators on token generation: REINFORCE, REBAR, and RELAX. 

  \end{abstract}
  
  \thispagestyle{equalc}
  \section{Introduction}
  The task of meaningful text generation is of crucial interest. To generate long sequences that make sense from end to end, a machine learning model must learn to keep and manage a lot of context and abstract features. In the past years, RNN-based neural networks have been trained successfully to predict the next character or word from a sequence of previous characters or words. However, directly producing full sequences, in natural language for instance, remains an active research area. 
  
In the last few years, Generative Adversarial Networks (GANs) have shown great performance on several generation tasks, especially image generation.  However, less work has been done regarding the task of generating sequential data, mainly because of two structural aspects of GANs. First, in GANs, the generator updates by backpropagating gradients from the outputs of the discriminator, which makes
little sense when the generated data is made of discrete tokens. Secondly, GANs can only give a score once an entire sequence has been generated, which is a problem with large sequences of tokens.

To alleviate these two issues, SeqGAN takes a reinforcement learning approach while generating tokens, and considers the generative model as a stochastic parametrized policy. This way, the gradient can pass back from the discriminator loss to the generatorâ€™s weights. When a token is generated, the
rest of the sequence is drawn with Monte Carlo sampling, so that the model bypasses the need to wait for a full sequence generation to get a score. It can get a score at each token generation via averaging these Monte Carlo samples. The reward is given by the discriminator's output when fed with the generated sequence. In this paper, we follow the same approach and use the discriminator as our reward function. 
  
  \section{Related Work}
  
  \section{Gradient Estimators}
      
  \subsection{REINFORCE}
  
  \subsection{REBAR}
  
  \subsection{RELAX}
  
  \section{SeqGAN with policy gradient}
  
  \section{Backpropagation through time}
  
  \section{Evaluation}
  
  \section{Experiments}
  
  \subsection{Toy data}
  
  \subsection{Pre-training with MLE}
  
  \subsection{Comparison of gradient estimators}
  
  \subsection{Quality of generated sequences}
  
  \subsection{Training schedule}
  
  
  \section{Conclusion}
  

  \bibliographystyle{plain}
  \bibliography{ref}
  
  
  \end{document}
